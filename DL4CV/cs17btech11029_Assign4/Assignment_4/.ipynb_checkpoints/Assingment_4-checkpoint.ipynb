{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3z9l7S6RV8-2"
   },
   "source": [
    "#### **Welcome to Assignment 4 (part-1) on Deep Learning for Computer Vision.**\n",
    "In this assignment you will get a chance to implement LSTM cell from Scratch and Usage of Recurrent Neural Network for 1D time series Prediction task .\n",
    "\n",
    "#### **Instructions**\n",
    "1. Use Python 3.x to run this notebook\n",
    "3. Write your code only in between the lines 'YOUR CODE STARTS HERE' and 'YOUR CODE ENDS HERE'.\n",
    "you should not change anything else code cells, if you do, the answers you are supposed to get at the end of this assignment might be wrong.\n",
    "4. Read documentation of each function carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GT8skzfOLu91"
   },
   "source": [
    "## Question 1:\n",
    "\n",
    "Given a sequence of values of a 1D input time series from time t = 1 to t = 5, predict the value of the time series at t = 6 using RNN.\n",
    "\n",
    "Here we trained an RNN in such a way that, given values of input time series from t = 1 to t=i ; it will predict the value at t= i+1.\n",
    "\n",
    "Hint : Design an RNN using pytorch's nn.RNN to create an RNN layer , then add a fully-connected layer to get the required output size.\n",
    "\n",
    "Choose 32 as the number of features in the RNN output and in the hidden state. Also, choose number of layers to be 1 to make up the RNN, typically such number varies depending on different tasks. The value greater than 1 means that you'll create a stacked RNN. Also, use \"batch_first =True\". Here, \"batch_first\" implies whether or not the input/output of the RNN will have the batch_size as the first dimension (batch_size, seq_length, hidden_dim). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qoVHp8ZWqYpd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "## Fixing the seed for Reproducibility\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "## Define 1D input time series, which spans from t= 1 to t=6.\n",
    "input_series = np.random.randn(6,1)\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        ### YOUR CODE STARTS HERE\n",
    "\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### YOUR CODE ENDS HERE\n",
    "\n",
    "# decide on hyperparameters\n",
    "input_size=1    ## 1D input\n",
    "output_size=1   ## 1D output\n",
    "hidden_dim=32  ## Hidden state feature dimension of RNN\n",
    "n_layers=1     ## No. of stacked layers in RNN\n",
    "\n",
    "# instantiate an RNN\n",
    "rnn = RNN(input_size, output_size, hidden_dim, n_layers)\n",
    "\n",
    "# MSE loss and Adam optimizer with a learning rate of 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.01)\n",
    "\n",
    "# train the RNN\n",
    "def train(rnn, n_steps, print_every):\n",
    "    \n",
    "    # initialize the hidden state\n",
    "    hidden = None      \n",
    "    \n",
    "    for batch_i, step in enumerate(range(n_steps)):\n",
    "        # defining the training data \n",
    "        x = input_series[:-1]\n",
    "        y = input_series[1:]\n",
    "        \n",
    "        # convert data into Tensors\n",
    "        x_tensor = torch.Tensor(x).unsqueeze(0) # unsqueeze gives a 1, batch_size dimension\n",
    "        y_tensor = torch.Tensor(y)\n",
    "\n",
    "        # outputs from the rnn\n",
    "        prediction, hidden = rnn(x_tensor, hidden)\n",
    "\n",
    "        ## Representing Memory ##\n",
    "        # make a new variable for hidden and detach the hidden state from its history\n",
    "        # this way, we don't backpropagate through the entire history\n",
    "        hidden = hidden.data\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = criterion(prediction, y_tensor)\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # perform backprop and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # display loss and predictions\n",
    "        if batch_i%print_every == 0:  \n",
    "            print (batch_i)      \n",
    "            print('Loss: ', loss.item())\n",
    "            print ('Predicted Value: ', prediction.data.numpy().flatten())\n",
    "            print ('True Value: ', y_tensor.data.numpy().flatten())\n",
    "            \n",
    "    \n",
    "    return rnn,prediction[-1]\n",
    "\n",
    "# train the rnn and monitor results\n",
    "trained_rnn,final_prediction = train(rnn, n_steps = 75, print_every= 11)\n",
    "print ('Final predicted value of input time series at t=6: ',final_prediction.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zfAVRvIFjxh"
   },
   "source": [
    "## Question 2:\n",
    "\n",
    "Given a Multivariate input time sequence and all the trainable parameters of LSTM Cell; Implement all the functionalities of the LSTM cell in order to predict the hidden state and output at time=t; given LSTM \"cell state\" at previous time step (t= t-1), LSTM \"hidden state\" at previous time step ( t= t-1) and the input at time=t. Hint : Follow the following sets of equation for implementing the functionality of LSTM Cell.\n",
    "\n",
    "Forget GATE: $f_{t} = \\sigma(W_{f}[ a_{t-1} ; x_{t}]  + b_{f}) $ (Note that \";\" denotes contatenation operation.)\n",
    "\n",
    "Update GATE: $i_{t} = \\sigma(W_{i}[ a_{t-1} ; x_{t} ] + b_i )$\n",
    "\n",
    "Memory GATE: $\\tilde{c}_{t} = tanh(W_c[ a_{t-1} ; x_{t} ] + b_c )$\n",
    "            update step -> $c_{t} =  f_{t} * c_{t-1} + i_{t} * \\tilde{c}_{t}$  (This operation determines how much information to keep from past and how much to add from current step information)\n",
    "\n",
    "Output GATE: $o_{t} = \\sigma(W_o [ a_{t-1} ; x_{t} ] + b_o)$\n",
    "           Final Output: $a_{t} = o_{t}*tanh(c_t) $\n",
    "( Note: For implementing \"tanh\" operation; use numpy.tanh libary function)\n",
    "\n",
    "\n",
    "a> Compute the value of a specific component of LSTM cell \"Output\" (y), i.e. y[1, 3, 4]? \n",
    "\n",
    "b> Also find the value of a specific component of LSTM hidden state output(a) ; i.e. a[2,1,5].\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOok6KSeaJ1o"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "np.random.seed(2)\n",
    "\n",
    "## Function implements Sigmoid Activation\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "## Function implements Softmax Activation\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "## Function implements LSTM \"forward pass\" of a single time step..i.e. given x at time step t, hidden state \n",
    "##at previous time step Memory state at previous time step , this function computes predicted output y at time step t. \n",
    "\n",
    "def lstm_forward_pass(xt, a_prev, c_prev, parameters):\n",
    "    \"\"\"\n",
    "    Implement a single forward step of the LSTM-cell \n",
    "\n",
    "    Arguments:\n",
    "    xt -- your input data at timestep \"t\"\n",
    "    a_prev -- Hidden state at timestep \"t-1\"\n",
    "    c_prev -- Memory state at timestep \"t-1\"\n",
    "\n",
    "    # Trainable Parameters of a LSTM cell\n",
    "    Wf -- Weight matrix of the forget gate; bf -- Bias of the forget gate\n",
    "    Wi -- Weight matrix of the update gate; bi -- Bias of the update gate\n",
    "    Wc -- Weight matrix of the first \"tanh\"; bc --  Bias of the first \"tanh\"\n",
    "    Wo -- Weight matrix of the output gate; bo --  Bias of the output gate\n",
    "    Wy -- Weight matrix relating the hidden-state to the output; by -- Bias relating the hidden-state to the output\n",
    "                        \n",
    "    The LSTM Cell MUST Returns:\n",
    "    a_next -- next hidden state\n",
    "    c_next -- next memory state\n",
    "    yt_pred -- LSTM output prediction at timestep \"t\"\n",
    "    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)\n",
    "    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),\n",
    "          c stands for the memory value\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve parameters from \"parameters\"\n",
    "    Wf = parameters[\"Wf\"]; bf = parameters[\"bf\"]\n",
    "    Wi = parameters[\"Wi\"]; bi = parameters[\"bi\"]\n",
    "    Wc = parameters[\"Wc\"]; bc = parameters[\"bc\"]\n",
    "    Wo = parameters[\"Wo\"]; bo = parameters[\"bo\"]\n",
    "    Wy = parameters[\"Wy\"]; by = parameters[\"by\"]\n",
    "    \n",
    "    # Retrieve dimensions from shapes of xt and Wy\n",
    "    n_x, m = xt.shape\n",
    "    n_y, n_a = Wy.shape\n",
    "\n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "    \n",
    "\n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "\n",
    "    # store values needed for backward propagation in cache\n",
    "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
    "\n",
    "    return a_next, c_next, yt_pred, cache\n",
    "\n",
    "def lstm_forward(x, a0, parameters):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation of the recurrent neural network using an LSTM-cell.\n",
    "\n",
    "    Arguments:\n",
    "    x -- Input data for every time-step\n",
    "    a0 -- Initial hidden state of LSTM cell\n",
    "    parameters \n",
    "    Wf -- Weight matrix of the forget gate ;bf -- Bias of the forget gate\n",
    "    Wi -- Weight matrix of the update gate ;bi -- Bias of the update gate\n",
    "    Wc -- Weight matrix of the first \"tanh\";bc -- Bias of the first \"tanh\"\n",
    "    Wo -- Weight matrix of the output gate; bo -- Bias of the output gate\n",
    "    Wy -- Weight matrix relating the hidden-state to the output; by -- Bias relating the hidden-state to the output\n",
    "                        \n",
    "    This Function call MUST Returns:\n",
    "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
    "    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
    "    c -- Memory states for every time-step\n",
    "    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize \"caches\", which will track the list of all the caches\n",
    "    caches = []\n",
    "    \n",
    "    ### YOUR CODE STARTS HERE ###\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE ###\n",
    "\n",
    "    # store values needed for backward propagation in cache\n",
    "    caches = (caches, x)\n",
    "    \n",
    "\n",
    "    return a, y, c, caches\n",
    "\n",
    "# Input time sequence\n",
    "x = np.random.randn(3,10,7)\n",
    "\n",
    "# Initial Hidden state of LSTM\n",
    "a0 = np.random.randn(5,10)\n",
    "\n",
    "# Weight and Bias Parameters of FORGET gate\n",
    "Weight_f = np.random.randn(5, 8); bias_f = np.random.randn(5,1)\n",
    "\n",
    "# Weight and Bias Parameters of UPDATE gate\n",
    "Weight_i = np.random.randn(5, 8); bias_i = np.random.randn(5,1)\n",
    "\n",
    "# Weight and Bias Parameters of OUTPUT gate\n",
    "Weight_o = np.random.randn(5, 8); bias_o = np.random.randn(5,1)\n",
    "\n",
    "# Weight and Bias Parameters of MEMORY gate (updating the cell)\n",
    "Weight_c = np.random.randn(5, 8); bias_c = np.random.randn(5,1)\n",
    "\n",
    "# Weight and bias for transforming hidden state output to final LSTM output for downstream application\n",
    "Weight_y = np.random.randn(2,5); bias_y = np.random.randn(2,1)\n",
    "\n",
    "LSTM_parameters = {\"Wf\": Weight_f, \"Wi\": Weight_i, \"Wo\": Weight_o, \"Wc\": Weight_c, \"Wy\": Weight_y, \"bf\": bias_f, \"bi\": bias_i, \"bo\": bias_o, \"bc\": bias_c, \"by\": bias_y}\n",
    "\n",
    "a, y, c, caches = lstm_forward(x, a0, LSTM_parameters)\n",
    "\n",
    "## Print the specific component value of LSTM cell \"Output\" (y) ;i.e. y[1,3,4]\n",
    "print(\"y[1][3][4] =\", y[1][3][4])\n",
    "print(\"y.shape = \", y.shape)\n",
    "\n",
    "## Print the specific component value of LSTM \"hidden state\" Output (a) ;i.e. a[2,1,5]\n",
    "print(\"a[2][1][5] = \", a[2][1][5])\n",
    "print(\"a.shape = \", a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SQRxSU9Tkwx"
   },
   "source": [
    "## Question 3:\n",
    "\n",
    "Time series prediction using recurrent models. \n",
    "\n",
    "Use the airline-passengers.csv file for this task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5e5Athrzw-DA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "training_set = pd.read_csv('airline-passengers.csv')\n",
    "#training_set = pd.read_csv('shampoo.csv')\n",
    "\n",
    "training_set = training_set.iloc[:,1:2].values\n",
    "\n",
    "#plt.plot(training_set, label = 'Shampoo Sales Data')\n",
    "plt.plot(training_set, label = 'Airline Passangers Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qo-Bn3vxGvD"
   },
   "source": [
    "## Prepare the training and testing dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCFod5yYxQXt"
   },
   "outputs": [],
   "source": [
    "def sliding_windows(data, seq_length):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(data)-seq_length-1):\n",
    "        _x = data[i:(i+seq_length)]\n",
    "        _y = data[i+seq_length]\n",
    "        x.append(_x)\n",
    "        y.append(_y)\n",
    "\n",
    "    return np.array(x),np.array(y)\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "training_data = sc.fit_transform(training_set)\n",
    "\n",
    "seq_length = 4\n",
    "x, y = sliding_windows(training_data, seq_length)\n",
    "\n",
    "train_size = int(len(y) * 0.67)\n",
    "test_size = len(y) - train_size\n",
    "\n",
    "dataX = Variable(torch.Tensor(np.array(x)))\n",
    "dataY = Variable(torch.Tensor(np.array(y)))\n",
    "\n",
    "trainX = Variable(torch.Tensor(np.array(x[0:train_size])))\n",
    "trainY = Variable(torch.Tensor(np.array(y[0:train_size])))\n",
    "\n",
    "testX = Variable(torch.Tensor(np.array(x[train_size:len(x)])))\n",
    "testY = Variable(torch.Tensor(np.array(y[train_size:len(y)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6-fbsv6_xVhu"
   },
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KC_5L1NIxUsR"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        # Your code goes here\n",
    "        # define a lstm block and a fc block\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        # Your code goes here\n",
    "        # Define a forward function for the LSTM block\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQKOnThFxxuJ"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vlj9F1Z6xxFR"
   },
   "outputs": [],
   "source": [
    "num_epochs = 2000\n",
    "learning_rate = 0.01\n",
    "\n",
    "input_size = 1\n",
    "hidden_size = 2\n",
    "num_layers = 1\n",
    "\n",
    "num_classes = 1\n",
    "\n",
    "lstm = LSTM(num_classes, input_size, hidden_size, num_layers)\n",
    "\n",
    "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Your code goes here\n",
    "    # define a training iteration to the lstm model. Use loss variable to compute the training loss\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "      print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gF0UsfR8yFrO"
   },
   "source": [
    "## Test and Visualize the results. \n",
    "Plot the output of time series similar to the data plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7-eERYByJ3O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPTjxV0G6g8X"
   },
   "source": [
    "#### **Welcome to Assignment 4 (part-2) on Deep Learning for Computer Vision.**\n",
    "This question consists of two subsection. In subsection-1 you'll have to code a Siamese Network, for subsection-2 you need to go through a official PyTorch tutorial on Object Detection, understand it and answer some questions.\n",
    "  \n",
    "#### **Instructions**\n",
    "1. Use Python 3.x to run this notebook\n",
    "2. Write your code only in between the lines 'YOUR CODE STARTS HERE' and 'YOUR CODE ENDS HERE'.\n",
    "you should not change anything else in the code cells, if you do, the answers you are supposed to get at the end of this assignment might be wrong.\n",
    "3. Read documentation of each function carefully.\n",
    "4. All the Best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJwH6jxrqI-5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import BatchSampler\n",
    "from torch.optim import lr_scheduler\n",
    "from PIL import Image\n",
    "import timeit\n",
    "\n",
    "## Please DONOT remove these lines. \n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)\n",
    "########################\n",
    "\n",
    "#### YOUR CODE STARTS HERE ####\n",
    "# Check availability of GPU and set the device accordingly\n",
    "device = \n",
    "#### YOUR CODE ENDS HERE ####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhSvcqdYqJ6U"
   },
   "source": [
    "#### Prepare the dataset for Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stYbGPoLqzDE"
   },
   "outputs": [],
   "source": [
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        \n",
    "        self.train = train\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # Define a set of transforms for preparing the dataset\n",
    "        self.transform =  # convert the image to a pytorch tensor\n",
    "                          # normalise the images with mean and std of the dataset\n",
    "        \n",
    "        # Load the MNIST training, test datasets using `torchvision.datasets.MNIST\n",
    "        # Set the train parameter to self.train and transform parameter to self.transform\n",
    "        self.dataset = \n",
    "\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        if self.train:\n",
    "            #### YOUR CODE STARTS HERE ####\n",
    "            # assign input (x-values) of training data \n",
    "            self.train_data =\n",
    "            # assign labels of training data \n",
    "            self.train_labels = \n",
    "            # get the set of all the labels in the dataset\n",
    "            self.labels_all = \n",
    "            self.label_to_idx = {} # assign a unique index to all labels in the dataset and store them in a dictionary \n",
    "\n",
    "            #### YOUR CODE ENDS HERE ####\n",
    "        else:\n",
    "            #### YOUR CODE STARTS HERE ####\n",
    "            # assign input (x-values) of test data \n",
    "            self.test_data = \n",
    "            # assign labels of test data \n",
    "            self.test_labels = \n",
    "            # get the set of all the labels in the dataset\n",
    "            self.labels_all = \n",
    "            self.label_to_idx = {} # assign a unique index to all labels in the dataset and store them in a dictionary \n",
    "\n",
    "            #### YOUR CODE ENDS HERE ####\n",
    "            # DONOT change this line of code  \n",
    "            random_state = np.random.RandomState(0)\n",
    "\n",
    "            positive_samples = [] # this will be a list of lists\n",
    "            for ind in range(0, len(self.test_data), 2):\n",
    "              positive_samples.append([ind, random_state.choice(self.label_to_idx[self.test_labels[ind].item()]), 1])\n",
    "            \n",
    "            negative_samples = []\n",
    "            for ind in range(1, len(self.test_data), 2):\n",
    "              negative_samples.append([ind, random_state.choice(self.label_to_idx[np.random.choice(\n",
    "                                                           list(self.labels_all - set([self.test_labels[ind].item()])))]), 0])\n",
    "            \n",
    "            # combine both positive and negative samples into a single variable\n",
    "            #### YOUR CODE STARTS HERE ####\n",
    "            self.test_samples = \n",
    "            #### YOUR CODE ENDS HERE ####\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # get image pair, and the associated label (1 for similar, 0 for dissimilar)\n",
    "        if self.train:\n",
    "            #### YOUR CODE STARTS HERE ####\n",
    "            # choose if training sample is similar or dissimilar\n",
    "            target = \n",
    "            # choose two images call them `first_image` and `second_image`\n",
    "            \n",
    "            #### YOUR CODE ENDS HERE ####\n",
    "        else:\n",
    "            #### YOUR CODE STARTS HERE ####\n",
    "            # select two images from self.test_samples, call them `first_image` and `second_image`\n",
    "            first_image = \n",
    "            second_image = \n",
    "            target = \n",
    "            #### YOUR CODE ENDS HERE ####\n",
    "        \n",
    "        first_image = Image.fromarray(first_image.numpy(), mode='L')\n",
    "        second_image = Image.fromarray(second_image.numpy(), mode='L')\n",
    "        first_image = self.transform(first_image)\n",
    "        second_image = self.transform(second_image)\n",
    "        return (first_image, second_image), target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gEE-dEarnvg"
   },
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # Define a sequential block as per the instructions below:\n",
    "        # Build three blocks with each block containing: Conv->PReLU->Maxpool layers\n",
    "        # Three conv layers should have 16, 32, 64 output channels respectively\n",
    "        # Use convolution kernel size 3\n",
    "        # For maxpool use a kernel size of 2 and stride of 2\n",
    "\n",
    "        self.convnet = \n",
    "\n",
    "\n",
    "        # Define linear->PReLU->linear->PReLU->linear\n",
    "        # The first two linear layers should have 256 and 128 output nodes\n",
    "        # The final FC layer should have 2 nodes\n",
    "        self.fc =\n",
    "\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "\n",
    "    def forward(self, x):\n",
    "      #### YOUR CODE STARTS HERE ####\n",
    "        # Define the forward pass, convnet -> fc\n",
    "        output = \n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPIClNjsrz78"
   },
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_net):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Call the embedding network for both the inputs and return the output\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        op1 = \n",
    "        op2 = \n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        return op1, op2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzlZzVgmM3hC"
   },
   "source": [
    "Pairwise contrastive loss\n",
    "$$\n",
    "L\\left(x_{0}, x_{1}, y\\right)=\\frac{1}{2} y\\left\\|f\\left(x_{0}\\right)-f\\left(x_{1}\\right)\\right\\|_{2}^{2}+\\frac{1}{2}(1-y)\\left\\{\\max (0, m-\\sqrt{\\|f(x_{0})-f(x_{1})\\|_{2}^{2} + \\epsilon)})\\right\\}^{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BclsdWZSr4RK"
   },
   "outputs": [],
   "source": [
    "class ContrastiveLossSiamese(nn.Module):\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(ContrastiveLossSiamese, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.eps = 1e-9\n",
    "\n",
    "    def forward(self, output1, output2, target):\n",
    "        # Use the equation mentioned above to define the loss\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        loss_value = \n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        loss_value = loss_value.mean()\n",
    "\n",
    "        return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVrUkFLmca1I"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, device, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        target = target if len(target) > 0 else None\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # send the image, target to the device\n",
    "        # data is not a single value here,\n",
    "        # ensure datatype of variable `data` is tuple\n",
    "        data = \n",
    "        target = \n",
    "        # flush out the gradients stored in optimizer\n",
    "\n",
    "        # pass the image to the model and assign the output to variable named outputs\n",
    "        # python star operator will be useful here\n",
    "        # if the datatype of outputs is not a tuple, make it to a tuple\n",
    "\n",
    "        outputs = \n",
    "\n",
    "        # create inputs to the contrastive loss (datatype should be tuple)\n",
    "        # calculate the loss using criterion \n",
    "        loss = \n",
    "        # append the loss to losses list and update the total_loss variable\n",
    "\n",
    "        # do a backward pass\n",
    "\n",
    "        # update the weights\n",
    "\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "\n",
    "        if batch_idx % 20 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data[0]), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), np.mean(losses)))  \n",
    "    total_loss /= (batch_idx + 1)\n",
    "    print('Average loss on training set: {:.6f}'.format(total_loss))\n",
    "\n",
    "def test(model, test_loader, device, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "          target = target if len(target) > 0 else None\n",
    "          #### YOUR CODE STARTS HERE ####\n",
    "          # send the image, target to the device\n",
    "          # data is not a single value here,\n",
    "          # ensure datatype of variable `data` is tuple\n",
    "          data = \n",
    "          target = \n",
    "          # pass the image to the model and assign the output to variable named outputs\n",
    "          # python star operator will be useful here\n",
    "          # if the datatype of outputs is not a tuple, make it to a tuple\n",
    "          outputs =\n",
    "\n",
    "          # create inputs to the contrastive loss\n",
    "          # datatype of target should be tuple\n",
    "          # calculate the loss\n",
    "          loss = \n",
    "          # update the test_loss variable\n",
    "          \n",
    "          #### YOUR CODE ENDS HERE ####\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    print('Average loss on test set: {:.6f}'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDZAVVWVcAC5"
   },
   "outputs": [],
   "source": [
    "# define the training and test sets\n",
    "# use SiameseDataset\n",
    "train_dataset = \n",
    "test_dataset =\n",
    "\n",
    "# create dataloaders for training and test datasets\n",
    "# use a batch size of 128 and set shuffle=True for the training set, set num_workers to 2 and pin_memory to True\n",
    "train_dataloader = \n",
    "test_dataloader = \n",
    "\n",
    "margin = 1.\n",
    "# create a instance of the embedding network and pass it as input to Siamese network\n",
    "embedding_net = \n",
    "model = \n",
    "# define the contrative loss with the specified margin\n",
    "criterion = \n",
    "optimizer = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CejgunVGzJPK"
   },
   "outputs": [],
   "source": [
    "start = timeit.default_timer()\n",
    "for epoch in range(1, 5):\n",
    "  train(model, train_dataloader, device, optimizer, criterion, epoch)\n",
    "  test(model, test_dataloader, device, criterion)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Total time taken: {} seconds'.format(int(stop - start)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5S0vb38a_o_r"
   },
   "source": [
    "### Question 4\n",
    "\n",
    "Run the code cell above and plot(on the same graph) the average train and test losses w.r.t epochs trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vgyN14PN5npM"
   },
   "source": [
    "### Question 5\n",
    "\n",
    "We have looked at training a Siamese Network with a pairwise contrastive loss. How would you modify the architecture above to use a triplet loss? The triplet loss is a ranking loss that uses image triplets consisting of an anchor $x_a$, a positive sample(similar to the anchor image) $x_p$ and a negative sample(dissimilar to the anchor image) $x_n$. Given triplet $(x_a, x_p, x_n)$ the teiplet loss is:\n",
    "\n",
    "$$\n",
    "L\\left(x_{a}, x_{p}, x_{n}\\right)=\\max \\left(0, m + \\|f(x_{a})-f(x_{p})\\|_{2}^{2} - \\|f(x_{a})-f(x_{n})\\|_{2}^{2}\\right)\n",
    "$$\n",
    "\n",
    "1. Write the dataloader to get triplets.\n",
    "2. Write code for class TripletLossSiamese. (the triplet loss)\n",
    "3. Describe in words how would you modify the network architecture to train it with a triplet loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1UhDPNO5npM"
   },
   "outputs": [],
   "source": [
    "# Question 2.1, dataloader for triplets\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        \n",
    "        self.train = train\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        # Define a set of transforms for preparing the dataset\n",
    "        self.transform =  # convert the image to a pytorch tensor\n",
    "                          # normalise the images with mean and std of the dataset\n",
    "        \n",
    "        # Load the MNIST training, test datasets using `torchvision.datasets.MNIST\n",
    "        # Set the train parameter to self.train and transform parameter to self.transform\n",
    "        self.dataset = \n",
    "\n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        if self.train:\n",
    "            #### YOUR CODE STARTS HERE ####\n",
    "            # assign input (x-values) of training data \n",
    "            self.train_data =\n",
    "            # assign labels of training data \n",
    "            self.train_labels = \n",
    "            # get the set of all the labels in the dataset\n",
    "            self.labels_all = \n",
    "            self.label_to_idx = {} # assign a unique index to all labels in the dataset and store them in a dictionary \n",
    "\n",
    "            #### YOUR CODE ENDS HERE ####\n",
    "        else:\n",
    "            #### YOUR CODE STARTS HERE ####\n",
    "            # assign input (x-values) of test data \n",
    "            self.test_data = \n",
    "            # assign labels of test data \n",
    "            self.test_labels = \n",
    "            # get the set of all the labels in the dataset\n",
    "            self.labels_all = \n",
    "            self.label_to_idx = {} # assign a unique index to all labels in the dataset and store them in a dictionary \n",
    "\n",
    "            #### YOUR CODE ENDS HERE ####\n",
    "            # DONOT change this line of code  \n",
    "            random_state = np.random.RandomState(0)\n",
    "            triplets = [] #this will be list of lists\n",
    "            for i in range(len(self.test_data)):\n",
    "                triplets.append([i, random_state.choice(self.label_to_idx[self.test_labels[i].item()]),\n",
    "                 random_state.choice(self.label_to_idx[np.random.choice(list(self.labels_all - set([self.test_labels[i].item()])))])\n",
    "                                ])\n",
    "            self.test_samples = triplets\n",
    "\n",
    "           \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # get image triplet\n",
    "        if self.train:\n",
    "            anchor, anchor_label = self.train_data[index], self.train_labels[index].item\n",
    "            #### YOUR CODE STARTS HERE ####\n",
    "            #choose positive and negative image call them `positive` and `negative` respectively\n",
    "            \n",
    "            #### YOUR CODE ENDS HERE ####\n",
    "        else:\n",
    "            #### YOUR CODE STARTS HERE ####\n",
    "            # select three images from self.test_samples\n",
    "            anchor = \n",
    "            positive = \n",
    "            negative = \n",
    "            #### YOUR CODE ENDS HERE ####\n",
    "        \n",
    "        anchor = Image.fromarray(anchor.numpy(), mode='L')\n",
    "        positive = Image.fromarray(positive.numpy(), mode='L')\n",
    "        negative = Image.fromarray(negative.numpy(), mode='L')\n",
    "        anchor = self.transform(anchor)\n",
    "        positive = self.transform(positive)\n",
    "        negative = self.transform(negative)\n",
    "        return (anchor, positive, negative), []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yz-LGiJN5npM"
   },
   "outputs": [],
   "source": [
    "# Question 2.2, define triplet loss\n",
    "class TripletLossSiamese(nn.Module):\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLossSiamese, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output_anchor, output_positive, output_negative):\n",
    "        # Use the triplet loss equation mentioned above to define the loss\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "        loss_value = \n",
    "        #### YOUR CODE ENDS HERE ####\n",
    "        loss_value = loss_value.mean()\n",
    "\n",
    "        return loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oP275cQFPvA"
   },
   "source": [
    "## Object Detection\n",
    "\n",
    "Go through the [Torchvision Object Detection Tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html) and ensure you understand the tutorial completely!\n",
    "\n",
    "After you have completely gone through the tutorial answer the following questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtaG0iORIXid"
   },
   "source": [
    "### Question 6\n",
    "\n",
    "Consider the metrics `AP@IoU=0.5` and `AP@IoU=0.75` used in the tutorial. Which of the following statements is True?  \n",
    "\n",
    "1. `IoU@0.75` will always be less than `IoU@0.5`\n",
    "2. `IoU@0.75` will always be  greater than `IoU@0.5` \n",
    "3. `IoU@0.75` need not be always be less than `IoU@0.5`\n",
    "4. `IoU@0.75` need not always be  greater than `IoU@0.5` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9yyqsuyPAMv"
   },
   "source": [
    "### Question 7\n",
    "\n",
    "Tutorial uses a network that is pre-trained on COCO dataset. Will training this model from scratch improve the performance? Provide justification for your answer. (Hint: You don't really have to re-train the model for this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71DVEZQf5hr8"
   },
   "source": [
    "### Question 8\n",
    "\n",
    "Write code to calculate IoU between aligned predicted bounding-boxes bbox_p and ground-truth bounding-boxes bbox_gt. Assume a co-ordinate system that has origin (0,0) at the upper-left corner of the image, and to the  right and down are +ve directions of x-axis and y-axis respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rngaSEln5npN"
   },
   "outputs": [],
   "source": [
    "def calculate_iou(bbox_p, bbox_gt):\n",
    "    #input: bbox_p and bbox_gt are (N,4) tensors\n",
    "    #output: ious (N,) vector\n",
    "    N = bbox_p.size(0)\n",
    "    \n",
    "    #### YOUR CODE STARTS HERE ####\n",
    "    # write code to compute the IoU between the bounding boxes\n",
    "    \n",
    "    #### YOUR CODE ENDS HERE ####\n",
    "    \n",
    "    return ious"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jR10OiXy6SkR"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assingment_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
